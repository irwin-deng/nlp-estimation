{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-models-official\n",
    "# !pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "from typing import Any\n",
    "\n",
    "# Only let TensorFlow allocate RAM as needed\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Configuration\n",
    "tf.random.set_seed(0)\n",
    "batch_size = 64\n",
    "seq_len = 128\n",
    "snli_ds_name = \"snli\"\n",
    "sentence_features = [\"premise\", \"hypothesis\"]\n",
    "num_classes = 3\n",
    "bert_encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "bert_preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_preprocessor(sentence_features: \"list[str]\", seq_length: int = 128) -> tf.keras.Model:\n",
    "    input_segments = [\n",
    "        tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "        for ft in sentence_features]\n",
    "\n",
    "    # Tokenize the text to word pieces.\n",
    "    bert_preprocess = hub.load(bert_preprocessor_url)\n",
    "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name=\"tokenizer\")\n",
    "    segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "    # Optional: Trim segments in a smart way to fit seq_length.\n",
    "    # Simple cases (like this example) can skip this step and let\n",
    "    # the next step apply a default truncation to approximately equal lengths.\n",
    "    truncated_segments = segments\n",
    "\n",
    "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "    # are model-dependent, so this gets loaded from the SavedModel.\n",
    "    packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                            arguments=dict(seq_length=seq_length),\n",
    "                            name=\"packer\")\n",
    "    model_inputs = packer(truncated_segments)\n",
    "    return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_dataset(full_dataset: \"dict[Any, tf.data.Dataset]\", info: tfds.core.DatasetBuilder, split: str,\n",
    "                          batch_size: int, preprocessor: tf.keras.Model) -> \"tuple[tf.data.Dataset, int]\":\n",
    "    is_training = split.startswith(\"train\")\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(full_dataset[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (preprocessor(ex), ex[\"label\"]))\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_classifier(num_classes: int) -> tf.keras.Model:\n",
    "    class Classifier(tf.keras.Model):\n",
    "        def __init__(self, num_classes):\n",
    "            super(Classifier, self).__init__(name=\"prediction\")\n",
    "            self.encoder = hub.KerasLayer(bert_encoder_url, trainable=True)\n",
    "            self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "            self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "        def call(self, preprocessed_text):\n",
    "            encoder_outputs = self.encoder(preprocessed_text)\n",
    "            pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "            x = self.dropout(pooled_output)\n",
    "            x = self.dense(x)\n",
    "            return x\n",
    "\n",
    "    model = Classifier(num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbt12\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py:566: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "bert_preprocessor = build_bert_preprocessor(sentence_features, seq_length=seq_len)\n",
    "\n",
    "# Create train, test, and validation data\n",
    "snli_ds: \"dict[tfds.Split, tf.data.Dataset]\" = tfds.load(snli_ds_name, batch_size=-1, shuffle_files=True)  # type: ignore\n",
    "snli_ds_info = tfds.builder(snli_ds_name).info\n",
    "\n",
    "train_data, train_data_size = get_data_from_dataset(snli_ds, snli_ds_info,\n",
    "    \"train\", batch_size, bert_preprocessor)\n",
    "steps_per_epoch = train_data_size // batch_size\n",
    "\n",
    "validation_data, validation_data_size = get_data_from_dataset(snli_ds, snli_ds_info,\n",
    "    \"validation\", batch_size, bert_preprocessor)\n",
    "validation_steps = validation_data_size // batch_size\n",
    "\n",
    "test_data, test_data_size = get_data_from_dataset(snli_ds, snli_ds_info,\n",
    "    \"test\", batch_size, bert_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 89s 535ms/step - loss: nan - accuracy: 0.3180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.3179999887943268]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\", dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Build model\n",
    "bert_classifier = build_bert_classifier(num_classes)\n",
    "bert_classifier.compile(optimizer=optimizer, metrics=metrics, loss=loss)\n",
    "\n",
    "# Evaluate model\n",
    "bert_classifier.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "66d5ff5d75b0aa59d8855ed66c860f9ea9884a6cb86de72b9dbdec822583f353"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
